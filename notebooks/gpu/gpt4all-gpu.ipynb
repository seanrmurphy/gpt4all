{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from nomic.gpt4all import GPT4AllGPU\n",
    "\n",
    "import textwrap \n",
    "wrap_column = 70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "# GPT4All will look in the .momic dir for the quantized weights\n",
    "LLAMA_PATH='/home/gltr/llama-7b-hf'\n",
    "m = GPT4AllGPU(LLAMA_PATH)\n",
    "config = {'num_beams': 2,\n",
    "          'min_new_tokens': 10,\n",
    "          'max_length': 100,\n",
    "          'repetition_penalty': 2.0}\n",
    "\n",
    "\n",
    "\n",
    "m = GPT4All()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: ggml ctx size = 6065.35 MB\n",
      "llama_model_load: memory_size =  2048.00 MB, n_mem = 65536\n",
      "llama_model_load: loading model part 1/1 from '/home/gltr/.nomic/gpt4all-lora-quantized.bin'\n",
      "llama_model_load: .................................... done\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n"
     ]
    }
   ],
   "source": [
    "# this loads the binary and the model - should really only be run once...\n",
    "m.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped(response):\n",
    "    wrapped_text = (textwrap.wrap(str(response),wrap_column))\n",
    "    for w in wrapped_text:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 << This is the prompt for your assignment as part of our AI writing\n",
      "competition. Your task here is to write us a short fiction piece that\n",
      "revolves around an isolated or abandoned computer, and explore its\n",
      "emotional state through dialogue with other characters in the world\n",
      "you create. Good luck!>>\n"
     ]
    }
   ],
   "source": [
    "output = m.generate('write me a story about a lonely computer', config)\n",
    "print_wrapped(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {“Swiss Army knives are made by Victorinox”} << This is true fact\n",
      "that can be used as an opening line for your Switzerland-themed short\n",
      "fiction piece, which should ideally revolve around the Swiss army and\n",
      "its iconic pocketknife.\n"
     ]
    }
   ],
   "source": [
    "output = m.prompt('tell me a story about switzerland' )\n",
    "print_wrapped(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {“Switzerland has a long history of producing high quality watches”}\n",
      "<< This is also true fact that can be used as an opening line for your\n",
      "Switzerland-themed short fiction piece, which should ideally revolve\n",
      "around the Swiss watch industry.\n"
     ]
    }
   ],
   "source": [
    "output = m.prompt('are swiss army knives computerized' )\n",
    "print_wrapped(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
